<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:20px;
		margin-left: auto;
		margin-right: auto;
		width: 900px;
	}
	
	h1 {
		font-size:28px;
		font-weight:300;
	}

	h2 {
		font-size: 24px;
		font-weight: 200;
	}
	
	.disclaimerbox {
		background-color: #eeeeee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Zeroth-Order Implicit Reinforcement Learning</title>
	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>



<body>
	<br>
	<center>
		<span style="font-size:30px">Zeroth-Order Implicit Reinforcement Learning for Sequential Decision Making in Distributed Control Systems</span>

		<tr>
			<td>

			</td>
		</tr>

		<table align=center width=850px>

			<table align=center width=850px>
				<br>
				<tr>
					<td align=center width=80px>
						<center>
							<span style="font-size:22px">Vanshaj Khattar</span>
						</center>
					</td>
					<td align=center width=80px>
						<center>
							<span style="font-size:22px">Qasim Wani</span>
						</center>
					</td>
					<td align=center width=80px>
						<center>
							<span style="font-size:22px">Harshal Kaushik</span>
						</center>
					</td>
					<td align=center width=80px>
						<center>
							<span style="font-size:22px">Zhiyao Zhang</span>
						</center>
					</td>
					<td align=center width=50px>
						<center>
							<span style="font-size:22px"><a href="http://www.jinming.tech">Ming Jin</a>*</span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=400px>
				<tr>
					<td align=center width=150px>
						<center>
							<span style="font-size:20px"><a href="http://www.jinming.tech/papers/ZO-iRL.pdf">[Paper]</a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:20px"><a href="https://github.com/QasimWani/ZO-iRL">[Code]</a></span><br>
							<!-- Zhiyao: pls confirm if the github repo is public or we share a package -->
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

<!--	<center>-->
<!--		<table align=center width=850px>-->
<!--			<tr>-->
<!--				<td width=260px>-->
<!--					<center>-->
<!--						<img class="round" style="width:500px" src="./resources/teaser.png"/>-->
<!--					</center>-->
<!--				</td>-->
<!--			</tr>-->
<!--		</table>-->
<!--		<table align=center width=850px>-->
<!--			<tr>-->
<!--				<td>-->
<!--					This was a template originally made for <a href="http://richzhang.github.io/colorization/">Colorful Image Colorization</a>. The code can be found in this <a href="https://github.com/richzhang/webpage-template">repository</a>.-->
<!--				</td>-->
<!--			</tr>-->
<!--		</table>-->
<!--	</center>-->

	<hr>

	<table align=center width=850px>
		<center><h1>I. Distributed Control Systems and Uncertainties</h1></center>
		<tr>
			<td>
				<p style="text-align: justify; text-justify: inter-ideograph">
					Sequantial decision making problems in DCSs widely exists in various domains,
				e.g., energy, supply chain management, finance, robotics. Uncertainties are common in those domains,
				which heavily bring difficulties in optimization and control, since algorithms are required capable of
				dealing with uncertainties in general. For example, as shown in <b>Fig.1</b>, in a micro grid energy management
				problem (we will show our experiments based on the CityLearn testbed in following content), end-users'
				energy demand (heating and cooling) are varying and can hardly be exactly predicted. Also,
				different buildings such as commercials and apartments exhibit various energy consumption patterns.
				In a supply chain management problem for profit maximization (an example structure in <b>Fig.2</b>),
				customers' future purchases are not clear to warehouse managers and sellers. In a more detailed expression,
				uncertainty may consist of values that can hardly be exactly predicted, probably concerning noise,
				the deviation of predictions and variables without strong indicators.
				</p>

			</td>
		</tr>
	</table>
	<br>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:700px" src="./resources/Fig1.png"/>
					</center>
					<tr>
						<td>
							<center>
								Fig.1  CityLearn 2020 energy management flowchart
									[<a href="https://arxiv.org/abs/2012.10504#">Vazquez-Canteli et al., 2020a</a>]</b>
							</center>
						</td>
					</tr>
				</td>
			</tr>
		</table>
	</center>
	<br>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:700px" src="./resources/Fig2.png"/>
					</center>
					<tr>
						<td>
							<center>
								Fig.2  An example of supply chain management model
									[<a href="https://doi.org/10.1016/S0098-1354(03)00047-4">Perea-Lopez et al., 2003</a>]</b>
							</center>
						</td>
					</tr>
				</td>
			</tr>
		</table>
	</center>

	<br>
	<hr>
	<center><h1>II. Why ZO-iRL?</h1></center>

	<table align=center width=800px>

		<tr>
			<td>
				<p style="text-align: justify; text-justify: inter-ideograph">
					Traditional optimization models, once built, cannot adapt to real-world changing conditions
				(or to say they are "rigid"). Although they are with solid theoretical foundations and broad
				industrial applications, due to their natural limitations, there still remains large room for
				optimization performance. Reinforcement Learning, a type of heuristic (learning-based) methods,
				is considered promising in recent studies. First, a predifined explicit system model is not required
				for optimization, so that the agent is capable of dealing with a large-scale, complex system.
				Moreover, by continuous training process, RL can systematically adapt to uncertainties.
				<br><br>
				The crux of our idea, ZO-iRL, is to combine the optimization models and RL to take both advantages.
				As shown in <b>Fig.3</b>, A convex optimization layer is consisted in the RL agent. It is implicit because
				the policy actions are implicitly defined w.r.t. optimization model parameters, the update of which
				also implicitly depends on rewards. The RL agent aims to adapt the parameters for convex optimization
				layer to dynamically changing system conditions, and the convex optimization enables the incorporation
				of general constraints into RL. We consider zeroth-order method instead of commonly-seen first-order
				one because of the lightweight computational capacity requirement that is easier to implement in general.
				</p>
			</td>
		</tr>
	</table>
	<br>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:500px" src="./resources/Fig3.jpg"/>
					</center>
					<tr>
						<td>
							<center>
								Fig.3 ZO-iRL basic architecture</b>
							</center>
						</td>
					</tr>
				</td>
			</tr>
		</table>
	</center>

	<br>
	<hr>
	<center><h1>III. Boosting the Training</h1></center>

	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<p style="text-align: justify; text-justify: inter-ideograph">
						Holding the assumption that in most DCS applications, parameters of corresponding optimization
						models do have clear physical interpretations, we design a guided random search mechanism that
						enables the search algorithm to leverage domain knowledge for faster and robust convergence.
						At each iteration, the algorithm randomly samples a set of candidates for exploration. The
						sampling distribution is determined by the noisy costs observed for the previous candidates.
						A guidance factor is introduced to guarantee the convergence. We use the guidance factor to guide
						the mean of the generated probability distributions towards the mean of best parameters in
						previous iterations.
						<b>(You can refer to the <a href="http://www.jinming.tech/papers/ZO-iRL.pdf">paper</a> for
							mathematical analysis and experiments
						on the guidance factor.)</b> In summary, the visualization of guidance mechanism is presented
						in <b>Fig.4</b>.

					</p>
				</td>
			</tr>
		</center>
	</table>

		<br>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:500px" src="./resources/Fig4.jpg"/>
					</center>
					<tr>
						<td>
							<center>
								Fig.4 Visualization of the guidance mechanism for ZO-iRL</b>
							</center>
						</td>
					</tr>
				</td>
			</tr>
		</table>
	</center>

	<br>
	<hr>
	<center><h1>IV. Experiments</h1></center>
	<center><h2>a. supply chain profit maximization</h2></center>

	<table align=center width=850px>

		<center>
			<tr>
				<td>
					<p style="text-align: justify; text-justify: inter-ideograph">
						We test the performance of ZO-iRL in a network of supply chains which contains 4 interconnected
						warehouses, 8 links over which goods can flow, 2 links that connect suppliers to warehouses,
						2 links that connect warehouses to customers, with the rest of the 4 links represent the
						internode connections. We follow the same cost function in
						[<a href="http://proceedings.mlr.press/v120/agrawal20a/agrawal20a.pdf">Agrawal et al., 2020</a>]

						and test the performance with the Convex Optimization Control Policy (COCP)
						in the original paper. Also, we introduce the noise in the cost function to manually
						inject uncertainties. You can see the difference of network topology with and without
						noise injection in the cost function in <b>Fig.5</b> and <b>Fig.6</b>. Comparisons of algorithm
						performance of ZO-iRL and COCP in terms of cost and convergence are shown in
						<b>Fig.7</b> and <b>Fig.8</b>, respectively representing the noiseless and noisy cost conditions.

					</p>
				</td>
			</tr>
		</center>
	</table>
		<br>
	<center>
		<table align=center width=850px>
			<tbody>
				<tr>
					<td width=350px>
						<center>
							<img class="round" style="width:300px" src="./resources/Fig5.png"/>
						</center>
						<br>
						<center>
							Fig.5 Topology of the supply chain. Left figure shows the initial untuned policy without noise. Right figure shows the tuned policy after 100 iterations.</b>
						</center>
					</td>
					<td width=350px>
						<center>
							<img class="round" style="width:300px" src="./resources/Fig6.png"/>
						</center>
						<br>
						<center>
							Fig.6 Topology of the supply chain. Left figure shows the initial untuned policy with noise. Right figure shows the tuned policy after 100 iterations.</b>
						</center>
					</td>
				</tr>
			</tbody>
		</table>
	</center>

	<br>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=350px>
					<center>
						<img class="round" style="width:300px" src="./resources/Fig7.png"/>
					</center>
					<br>
					<center>
						Fig.7 Comparison of learning curves in the setting of noiseless cost evaluation</b>
					</center>
				</td>
				<td width=350px>
					<center>
						<img class="round" style="width:300px" src="./resources/Fig8.png"/>
					</center>
					<br>
					<center>
						Fig.8 Comparison of learning curves in the setting of noisy cost evaluation.
					</center>
				</td>
			</tr>
		</table>
	</center>


	<br><center><h2>b. micro-grid energy flow management (CityLearn Challenge)</h2></center>

	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<p style="text-align: justify; text-justify: inter-ideograph">
						CityLearn Challenge is an annual competition aimed to inspire Multi-Agent RL (MARL)
						solutions to address grand challenges for the control of power grid.
						The competition has an online setup with only one episode of the entire 4 years,
						when agents will exploit the best policies to optimize the coordination strategy
						among 9 buildings and three energies (heating, cooling and electricity). The energy
						model in CityLearn environment buildings are shown in <b>Fig.9</b>. CityLearn Challenge
						consists of multiple scoring metrics (you can have a detailed look
						<a href="http://sites.google.com/view/citylearnchallenge">here</a>),
						and we compare ZO-iRL with other methods provided
						in the CityLearn environment shown in <b>Fig.10</b>. The learning curve w.r.t.
						baseline Rule-Based Controller (RBC) which keeps a standardized cost value constant
						to 1 is presented in <b>Fig.11</b>.

					</p>
				</td>
			</tr>
		</center>
	</table>
	<br>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:400px" src="./resources/Fig9.png"/>
					</center>
					<tr>
						<td>
							<center>
								Fig.9 CityLearn energy model. Chilled water storage and batteries are with all buildings, while DHW water storage and PV arrays are selective.</b>
							</center>
						</td>
					</tr>
				</td>
			</tr>
		</table>
	</center>
	<br>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:800px" src="./resources/Fig10.png"/>
					</center>
					<tr>
						<td>
							<center>
								Fig.10  Scores for ZO-iRL and comparison methods, including SAC
								[<a href="https://arxiv.org/pdf/2009.10562.pdf">Kathirgamanathan et al., 2020</a>] and MARLISA
								[<a href="https://dl.acm.org/doi/pdf/10.1145/3408308.3427604">Vazquez-Canteli et al., 2020b</a>].
								The random agent basically uniformly selects an action within the range at each timestep.
							</center>
						</td>
					</tr>
				</td>
			</tr>
		</table>
	</center>
	<br>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:400px" src="./resources/Fig11.png"/>
					</center>
					<tr>
						<td>
							<center>
								Fig.11 the learning curve of ZO-iRL with respect to RBC evaluated on a 4-month basis.
							</center>
						</td>
					</tr>
				</td>
			</tr>
		</table>
	</center>

	<br>
	<hr>
	<center><h1>V. Conclusion</h1></center>

	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<p style="text-align: justify; text-justify: inter-ideograph">
						We introduce a novel implicit RL framework for model-free sequential decision making in DCSs.
						The proposed method leverages the synergistic strength of convex optimization RL,
						thus becoming able to simultaneously address a range of challenges for real-world RL.
						This work broadens future research directions including the extension of the implicit
						RL framework to other derivative-free methods such as Bayesian optimization and
						first-order methods such as actor-critic.

					</p>
				</td>
			</tr>
		</center>
	</table>

	<hr>
	<br>

	<table align=center width=850px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					The authors would like to thank the organizers of the CityLearn Challenge
					(Dr. Jose R. Vazquez-Canteli, Dr. Zoltan Nagy, Dr. Gregor Henze, and Sourav Dey)
					for motivating the proposed approach.
					<br><br>
					This webpage is made from the template by
					<a href="http://web.mit.edu/phillipi/">Phillip Isola</a>
					and <a href="http://richzhang.github.io/">Richard Zhang</a>.
					The code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>


